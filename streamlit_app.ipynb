{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tree.utils import entropy, information_gain, gini_index, get_mse\n",
    "\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion: str, input_Dtype: str, output_Dtype: str, depth: int) -> None:\n",
    "        self.criterion = criterion\n",
    "        self.input_Dtype = input_Dtype\n",
    "        self.output_Dtype = output_Dtype\n",
    "        self.tree = {}\n",
    "        self.min_samples_split = 2\n",
    "        self.max_depth=10,\n",
    "        self.depth=depth\n",
    "        self.max_class = None\n",
    "        self.mean_class = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        if self.input_Dtype == \"Discrete\":\n",
    "            if self.output_Dtype == \"Discrete\":\n",
    "              self.max_class = y.value_counts().idxmax()\n",
    "            else:\n",
    "              self.mean_class = y.mean()\n",
    "            self.tree = self._build_tree_Discrete(X, y, 0)\n",
    "        else:\n",
    "            self.tree = self._build_tree_Real(X, y, 0)\n",
    "\n",
    "    def best_split(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Funtion to best split on real features\n",
    "        \"\"\"\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        if self.output_Dtype == \"Real\":\n",
    "            mse_base = get_mse(y)\n",
    "        else:\n",
    "            info_gain_base = float('-inf')\n",
    "        df = pd.merge(X, y, right_index=True, left_index=True)\n",
    "        for feature in list(X.columns):\n",
    "            Xdf = df.dropna().sort_values(feature)\n",
    "            xmeans = Xdf[feature].rolling(2).mean()\n",
    "            xmeans.dropna(inplace=True)\n",
    "            for value in xmeans:\n",
    "                left_y = Xdf[Xdf[feature] <= value][y.name]\n",
    "                right_y = Xdf[Xdf[feature] > value][y.name]\n",
    "                if self.output_Dtype == \"Real\":\n",
    "                    left_mean = left_y.mean()\n",
    "                    right_mean = right_y.mean()\n",
    "                    res_left = left_y - left_mean\n",
    "                    res_right = right_y - right_mean\n",
    "                    r = pd.concat([res_left, res_right], ignore_index=True)\n",
    "                    n = len(r)\n",
    "                    r = r ** 2\n",
    "                    r = r.sum()\n",
    "                    mse_split = r / n\n",
    "                    if mse_split < mse_base:\n",
    "                        best_feature = feature\n",
    "                        best_value = value\n",
    "                        mse_base = mse_split\n",
    "                else:\n",
    "                    info_gain = information_gain(pd.concat([left_y, right_y], ignore_index=True), pd.Series(\n",
    "                        [\"Yes\"] * len(left_y) + [\"No\"] * len(right_y), name=\"Y\"), criteria=self.criterion, input_Dtype=self.input_Dtype)\n",
    "                    if info_gain > info_gain_base:\n",
    "                        best_feature = feature\n",
    "                        best_value = value\n",
    "                        info_gain_base = info_gain\n",
    "\n",
    "        return best_feature, best_value\n",
    "\n",
    "    def _select_feature_Discrete(self, X: pd.DataFrame, y: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Funtion to find the best discrete feature\n",
    "        \"\"\"\n",
    "        info_gain = {}\n",
    "        for attr in X.columns:\n",
    "            info_gain[attr] = information_gain(\n",
    "                y, X[attr], self.criterion,  input_Dtype=self.input_Dtype)\n",
    "        return max(info_gain, key=lambda x: info_gain[x])\n",
    "\n",
    "    def _build_tree_Discrete(self, X: pd.DataFrame, y: pd.Series, depth: int):\n",
    "        \"\"\"\n",
    "        Funtion to build tree on discrete inputs\n",
    "        \"\"\"\n",
    "        if depth <= self.depth:\n",
    "            if y.nunique() == 1:\n",
    "                return y.iloc[0]\n",
    "            if len(X.columns) == 0:\n",
    "                return y.value_counts().idxmax()\n",
    "            best_feature = self._select_feature_Discrete(X, y)\n",
    "            tree = {best_feature: {}}\n",
    "            for value in X[best_feature].unique():\n",
    "                sub_X = X[X[best_feature] == value].drop(best_feature, axis=1)\n",
    "                sub_y = y[X[best_feature] == value]\n",
    "                if len(sub_X) > 0:\n",
    "                    tree[best_feature][value] = self._build_tree_Discrete(\n",
    "                        sub_X, sub_y, depth+1)\n",
    "                else:\n",
    "                    tree[best_feature][value] = y.value_counts().idxmax()\n",
    "        return tree\n",
    "\n",
    "    def _build_tree_Real(self, X: pd.DataFrame, y: pd.Series, depth: int):\n",
    "        \"\"\"\n",
    "        Funtion to build tree on real inputs\n",
    "        \"\"\"\n",
    "        if depth < self.depth and len(y) >= self.min_samples_split:\n",
    "            if self.output_Dtype == \"Real\":\n",
    "                if len(X.columns) == 0:\n",
    "                    return y.mean()\n",
    "            else:\n",
    "                if len(X.columns) == 0:\n",
    "                    return y.value_counts().idxmax()\n",
    "                if y.nunique() == 1:\n",
    "                    return y.iloc[0]\n",
    "            best_feature, best_value = self.best_split(X, y)\n",
    "            if best_feature is not None:\n",
    "                tree = {best_feature: {best_value: {}}}\n",
    "                left_X = X[X[best_feature] <= best_value]\n",
    "                right_X = X[X[best_feature] > best_value]\n",
    "                left_y = y[X[best_feature] <= best_value]\n",
    "                right_y = y[X[best_feature] > best_value]\n",
    "                tree[best_feature][best_value][\" <= \"] = self._build_tree_Real(\n",
    "                    left_X, left_y, depth+1)\n",
    "                tree[best_feature][best_value][\" > \"] = self._build_tree_Real(\n",
    "                    right_X, right_y, depth+1)\n",
    "\n",
    "                return tree\n",
    "        else:\n",
    "            if self.output_Dtype == \"Real\":\n",
    "                return y.mean()\n",
    "            else:\n",
    "                return y.value_counts().idxmax()\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Funtion to run the decision tree on test inputs\n",
    "        \"\"\"\n",
    "        current_node = self.tree\n",
    "        predictions = []\n",
    "        for i, row in X.iterrows():\n",
    "            current_node = self.tree\n",
    "            # Keep looping until you reach a leaf node\n",
    "            while(True):\n",
    "                if isinstance(current_node, dict):\n",
    "                    split_feature = list(current_node.keys())\n",
    "                    split_val = row[split_feature]\n",
    "                    if self.input_Dtype == \"Discrete\":\n",
    "                        if split_val.values[0] in list(current_node[split_feature[0]].keys()):\n",
    "                            current_node = current_node[split_feature[0]][split_val.values[0]]\n",
    "                        else:\n",
    "                            if self.output_Dtype == \"Discrete\":\n",
    "                              predictions.append(self.max_class)\n",
    "                            else:\n",
    "                              predictions.append(self.mean_class)\n",
    "                            break\n",
    "                    else:\n",
    "                        if split_val.values[0] <= list(current_node[split_feature[0]].keys())[0]:\n",
    "                            current_node = current_node[split_feature[0]][list(current_node[split_feature[0]].keys())[0]][\"L\"]\n",
    "                        else:\n",
    "                            current_node = current_node[split_feature[0]][list(current_node[split_feature[0]].keys())[0]][\"R\"]\n",
    "                else:\n",
    "                    predictions.append(current_node)\n",
    "                    break\n",
    "\n",
    "        return pd.Series(predictions)\n",
    "\n",
    "        return pd.Series(predictions)\n",
    "\n",
    "    def plot(self) -> None:\n",
    "        \"\"\"\n",
    "        Funtion to plot the decision tree\n",
    "        \"\"\"\n",
    "        print(json.dumps(self.tree, indent=4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def entropy(Y: pd.Series) -> float:\n",
    "    count_class = Y.value_counts()\n",
    "    prob_class = count_class/count_class.sum()\n",
    "    entropy = - prob_class * np.log2(prob_class)\n",
    "    return entropy.sum()\n",
    "\n",
    "def gini_index(Y: pd.Series) -> float:\n",
    "    count_class = Y.value_counts()\n",
    "    prob_class = count_class/count_class.sum()\n",
    "    gini_index = 1 - (prob_class**2).sum()\n",
    "    return gini_index\n",
    "\n",
    "\n",
    "def information_gain(Y: pd.Series, attr: pd.Series, criteria: str, input_Dtype: str) -> float:\n",
    "    Y = Y\n",
    "    parent_impurity = entropy(Y)\n",
    "    subsets_impurities = {}\n",
    "    subsets_size = {}\n",
    "    merged = pd.merge(attr, Y, right_index = True, left_index = True)\n",
    "    for value in attr.unique():\n",
    "        subset = merged.groupby(merged.columns[0]).get_group(value)[merged.columns[1]]\n",
    "        if criteria == 'gini_index':\n",
    "            subsets_impurities[value] = gini_index(subset)\n",
    "        elif criteria == \"entropy\":\n",
    "            subsets_impurities[value] = entropy(subset)\n",
    "        elif criteria == \"variance\":\n",
    "            subsets_impurities[value] = subset.var()\n",
    "        elif criteria == \"std\":\n",
    "            subsets_impurities[value] = subset.std()\n",
    "        subsets_size[value] = len(subset)/len(attr)\n",
    "    if criteria == 'gini_index' or criteria == \"variance\" or criteria == \"std\":\n",
    "        information_gain = sum(subsets_impurities[k]*subsets_size[k] for k in subsets_impurities)\n",
    "    else:\n",
    "        if input_Dtype == \"Discrete\":\n",
    "            information_gain = parent_impurity - sum(subsets_impurities[k]*subsets_size[k] for k in subsets_impurities)\n",
    "        else:\n",
    "            information_gain = -sum(subsets_impurities[k]*subsets_size[k] for k in subsets_impurities)\n",
    "    return information_gain\n",
    "\n",
    "def get_mse(Y: pd.Series) -> float:\n",
    "    n = len(Y)\n",
    "    r = Y - Y.mean() \n",
    "    r = r ** 2\n",
    "    r = r.sum()\n",
    "    return r / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use streamlit to create a web app that plot bias variance tradeoff with varyiing tree depth\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "st.title(\"Bias Variance Tradeoff\")\n",
    "st.write(\"This app plots the bias variance tradeoff for a decision tree model\")\n",
    "st.sidebar.header(\"User Input Parameters\")\n",
    "\n",
    "def user_input_features():\n",
    "    depth = st.sidebar.slider(\"Tree Depth\", 1, 10, 1)\n",
    "    return depth\n",
    "\n",
    "depth = user_input_features()\n",
    "test_size = 0.3\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=2, noise=20, random_state=1)\n",
    "df = pd.DataFrame(X)\n",
    "trg_col = 'Y'\n",
    "df[trg_col] = y\n",
    "num_data_pnts = len(df.index)\n",
    "df_train = df.loc[0:(1-test_size)*num_data_pnts-1]\n",
    "df_test = df.loc[(1-test_size)*num_data_pnts:num_data_pnts]\n",
    "dt = DecisionTree(df_train, df_test, trg_col, depth, \"Real\", \"Real\", depth=depth)\n",
    "dt.fit(df_train.drop([trg_col], axis = 1), df_train[trg_col])\n",
    "y_pred = dt.predict(df_test.drop([trg_col], axis = 1))\n",
    "mse = get_mse(y_pred - df_test[trg_col])\n",
    "st.write(\"MSE: \", mse)\n",
    "st.write(\"Bias: \", (df_test[trg_col].mean() - y_pred.mean())**2)\n",
    "st.write(\"Variance: \", y_pred.var())\n",
    "st.write(\"Bias + Variance: \", (df_test[trg_col].mean() - y_pred.mean())**2 + y_pred.var())\n",
    "\n",
    "st.write(\"The plot below shows the bias variance tradeoff for a decision tree model with varying tree depth\")\n",
    "st.write(\"The MSE is the sum of bias and variance\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
